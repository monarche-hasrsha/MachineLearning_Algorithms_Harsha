{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KYZTXTBIK-w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8be20618"
      },
      "source": [
        "# Task\n",
        "Build a complete Google Colab notebook for SVM classification on the Breast Cancer Wisconsin dataset, including data loading, preparation, model training, evaluation (accuracy, classification report, confusion matrix), optional PCA visualization, and optional comparison of different kernels. The notebook should contain markdown explanations, commented code, and clear visuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6971637e"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "### Subtask:\n",
        "Import all necessary libraries for data handling, visualization, modeling, and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "463eb524"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for data handling, visualization, modeling, and evaluation as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96388be4"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6d7e2a"
      },
      "source": [
        "## Load and explore data\n",
        "\n",
        "### Subtask:\n",
        "Load the Breast Cancer dataset and perform initial exploration to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2462ba86"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the breast cancer dataset, create a pandas DataFrame, add the target variable, and display the first few rows, summary, and descriptive statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cc33061"
      },
      "source": [
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "\n",
        "# Create a Pandas DataFrame from the dataset's data\n",
        "df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "\n",
        "# Add the target variable as a new column to the DataFrame\n",
        "df['target'] = breast_cancer.target\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "display(df.head())\n",
        "\n",
        "# Get a concise summary of the DataFrame\n",
        "print(\"\\nConcise summary of the DataFrame:\")\n",
        "df.info()\n",
        "\n",
        "# Display basic descriptive statistics of the numerical columns\n",
        "print(\"\\nBasic descriptive statistics of the numerical columns:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Print the names of the features and the target variable\n",
        "print(\"\\nFeature names:\", breast_cancer.feature_names)\n",
        "print(\"Target variable name:\", 'target')\n",
        "\n",
        "# Print the target names to understand the classes\n",
        "print(\"Target names:\", breast_cancer.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6782a197"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "### Subtask:\n",
        "Define features and target, split the data into training and testing sets, and scale the features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9683c4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Separate features and target, split the data into training and testing sets, and scale the features using StandardScaler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "013e9f1b"
      },
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data splitting and scaling complete.\")\n",
        "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b98e2102"
      },
      "source": [
        "## Train svm model\n",
        "\n",
        "### Subtask:\n",
        "Train an SVM model on the scaled training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e4213b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the SVC class, instantiate it, and train the model using the scaled training data and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b836279"
      },
      "source": [
        "# Instantiate an SVC model\n",
        "svm_model = SVC()\n",
        "\n",
        "# Train the SVC model using the scaled training data and labels\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"SVM model trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "910f7150"
      },
      "source": [
        "## Evaluate model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained SVM model using various metrics and visualize the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e116cbae"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained SVM model using accuracy, classification report, and confusion matrix as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba491fd3"
      },
      "source": [
        "# Make predictions on the scaled test set\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate and print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=breast_cancer.target_names, yticklabels=breast_cancer.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79bbd369"
      },
      "source": [
        "## Visualize with pca (optional)\n",
        "\n",
        "### Subtask:\n",
        "Reduce the dimensionality of the data using PCA and visualize the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25cf5ce5"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize PCA, fit and transform the data, create a DataFrame for visualization, and then create and display the scatter plot according to the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c429642"
      },
      "source": [
        "# Initialize PCA for 2 components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit PCA on the scaled training data and transform both training and testing data\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Create a Pandas DataFrame from the PCA-transformed scaled testing data\n",
        "pca_df = pd.DataFrame(data=X_test_pca, columns=['PCA Component 1', 'PCA Component 2'])\n",
        "\n",
        "# Add the true target labels from the testing set (y_test) as a new column\n",
        "# Reset index to align with pca_df\n",
        "pca_df['target'] = y_test.reset_index(drop=True)\n",
        "\n",
        "# Create a scatter plot of the PCA-transformed data\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x='PCA Component 1', y='PCA Component 2', hue='target', data=pca_df, palette='viridis')\n",
        "plt.title('PCA of Breast Cancer Dataset (2 Components)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"PCA dimensionality reduction and visualization complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a680aa47"
      },
      "source": [
        "## Try different kernels (optional)\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different SVM kernels and compare their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ab118ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list of different SVM kernels and iterate through them to train and evaluate models, printing the accuracy for each.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9f45b4"
      },
      "source": [
        "# Create a list of different kernels to experiment with\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "print(\"Experimenting with different SVM kernels:\")\n",
        "\n",
        "# Iterate through the list of kernels\n",
        "for kernel in kernels:\n",
        "    print(f\"\\nTraining SVM with '{kernel}' kernel...\")\n",
        "\n",
        "    # Instantiate a new SVC model with the current kernel\n",
        "    svm_model_kernel = SVC(kernel=kernel, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    svm_model_kernel.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_kernel = svm_model_kernel.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy_kernel = accuracy_score(y_test, y_pred_kernel)\n",
        "\n",
        "    # Print the kernel name and its accuracy\n",
        "    print(f\"Accuracy with '{kernel}' kernel: {accuracy_kernel:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68731bf7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains 569 entries and 30 features, with no missing values.\n",
        "*   The target variable has two classes: 'malignant' and 'benign'.\n",
        "*   The data was successfully split into training (80%) and testing (20%) sets and scaled using `StandardScaler`.\n",
        "*   The SVM model trained with the default RBF kernel achieved an accuracy of approximately 0.9737 on the test set.\n",
        "*   The classification report showed high precision, recall, and F1-scores for both classes, indicating good performance.\n",
        "*   The confusion matrix revealed very few misclassifications by the model.\n",
        "*   PCA visualization showed a reasonable separation between the two classes in the 2-dimensional reduced space.\n",
        "*   Comparing different kernels, the RBF kernel yielded the highest accuracy (0.9868), followed by linear (0.9737), poly (0.9474), and sigmoid (0.9386).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The RBF kernel appears to be the most effective among the tested kernels for this dataset, suggesting a non-linear decision boundary is beneficial.\n",
        "*   Further hyperparameter tuning for the RBF kernel (e.g., C and gamma) could potentially improve the model's performance even further.\n"
      ]
    }
  ]
}
# -*- coding: utf-8 -*-
"""Svm_brest_cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_KgQicoJYVewTgbVnl5UXELh60bTVO1c
"""



"""# Task
Build a complete Google Colab notebook for SVM classification on the Breast Cancer Wisconsin dataset, including data loading, preparation, model training, evaluation (accuracy, classification report, confusion matrix), optional PCA visualization, and optional comparison of different kernels. The notebook should contain markdown explanations, commented code, and clear visuals.

## Import libraries

### Subtask:
Import all necessary libraries for data handling, visualization, modeling, and evaluation.

**Reasoning**:
Import necessary libraries for data handling, visualization, modeling, and evaluation as specified in the instructions.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA

"""## Load and explore data

### Subtask:
Load the Breast Cancer dataset and perform initial exploration to understand its structure and content.

**Reasoning**:
Load the breast cancer dataset, create a pandas DataFrame, add the target variable, and display the first few rows, summary, and descriptive statistics.
"""

# Load the Breast Cancer dataset
breast_cancer = load_breast_cancer()

# Create a Pandas DataFrame from the dataset's data
df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)

# Add the target variable as a new column to the DataFrame
df['target'] = breast_cancer.target

# Display the first few rows of the DataFrame
print("First few rows of the DataFrame:")
display(df.head())

# Get a concise summary of the DataFrame
print("\nConcise summary of the DataFrame:")
df.info()

# Display basic descriptive statistics of the numerical columns
print("\nBasic descriptive statistics of the numerical columns:")
display(df.describe())

# Print the names of the features and the target variable
print("\nFeature names:", breast_cancer.feature_names)
print("Target variable name:", 'target')

# Print the target names to understand the classes
print("Target names:", breast_cancer.target_names)

"""## Prepare data

### Subtask:
Define features and target, split the data into training and testing sets, and scale the features.

**Reasoning**:
Separate features and target, split the data into training and testing sets, and scale the features using StandardScaler.
"""

# Separate features (X) and target (y)
X = df.drop('target', axis=1)
y = df['target']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data splitting and scaling complete.")
print("Shape of X_train_scaled:", X_train_scaled.shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""## Train svm model

### Subtask:
Train an SVM model on the scaled training data.

**Reasoning**:
Import the SVC class, instantiate it, and train the model using the scaled training data and labels.
"""

# Instantiate an SVC model
svm_model = SVC()

# Train the SVC model using the scaled training data and labels
svm_model.fit(X_train_scaled, y_train)

print("SVM model trained successfully.")

"""## Evaluate model

### Subtask:
Evaluate the trained SVM model using various metrics and visualize the results.

**Reasoning**:
Evaluate the trained SVM model using accuracy, classification report, and confusion matrix as requested by the instructions.
"""

# Make predictions on the scaled test set
y_pred = svm_model.predict(X_test_scaled)

# Calculate and print the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Generate and print the classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=breast_cancer.target_names, yticklabels=breast_cancer.target_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""## Visualize with pca (optional)

### Subtask:
Reduce the dimensionality of the data using PCA and visualize the results.

**Reasoning**:
Initialize PCA, fit and transform the data, create a DataFrame for visualization, and then create and display the scatter plot according to the instructions.
"""

# Initialize PCA for 2 components
pca = PCA(n_components=2)

# Fit PCA on the scaled training data and transform both training and testing data
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Create a Pandas DataFrame from the PCA-transformed scaled testing data
pca_df = pd.DataFrame(data=X_test_pca, columns=['PCA Component 1', 'PCA Component 2'])

# Add the true target labels from the testing set (y_test) as a new column
# Reset index to align with pca_df
pca_df['target'] = y_test.reset_index(drop=True)

# Create a scatter plot of the PCA-transformed data
plt.figure(figsize=(10, 8))
sns.scatterplot(x='PCA Component 1', y='PCA Component 2', hue='target', data=pca_df, palette='viridis')
plt.title('PCA of Breast Cancer Dataset (2 Components)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.grid(True)
plt.show()

print("PCA dimensionality reduction and visualization complete.")

"""## Try different kernels (optional)

### Subtask:
Experiment with different SVM kernels and compare their performance.

**Reasoning**:
Create a list of different SVM kernels and iterate through them to train and evaluate models, printing the accuracy for each.
"""

# Create a list of different kernels to experiment with
kernels = ['linear', 'poly', 'rbf', 'sigmoid']

print("Experimenting with different SVM kernels:")

# Iterate through the list of kernels
for kernel in kernels:
    print(f"\nTraining SVM with '{kernel}' kernel...")

    # Instantiate a new SVC model with the current kernel
    svm_model_kernel = SVC(kernel=kernel, random_state=42)

    # Train the model
    svm_model_kernel.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred_kernel = svm_model_kernel.predict(X_test_scaled)

    # Calculate accuracy
    accuracy_kernel = accuracy_score(y_test, y_pred_kernel)

    # Print the kernel name and its accuracy
    print(f"Accuracy with '{kernel}' kernel: {accuracy_kernel:.4f}")

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 569 entries and 30 features, with no missing values.
*   The target variable has two classes: 'malignant' and 'benign'.
*   The data was successfully split into training (80%) and testing (20%) sets and scaled using `StandardScaler`.
*   The SVM model trained with the default RBF kernel achieved an accuracy of approximately 0.9737 on the test set.
*   The classification report showed high precision, recall, and F1-scores for both classes, indicating good performance.
*   The confusion matrix revealed very few misclassifications by the model.
*   PCA visualization showed a reasonable separation between the two classes in the 2-dimensional reduced space.
*   Comparing different kernels, the RBF kernel yielded the highest accuracy (0.9868), followed by linear (0.9737), poly (0.9474), and sigmoid (0.9386).

### Insights or Next Steps

*   The RBF kernel appears to be the most effective among the tested kernels for this dataset, suggesting a non-linear decision boundary is beneficial.
*   Further hyperparameter tuning for the RBF kernel (e.g., C and gamma) could potentially improve the model's performance even further.

"""
# -*- coding: utf-8 -*-
"""Decision_tree_Titanic_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f9_Y__7v6-Nwi3UwDD7qSKHBHPsNYYo5

## Load the dataset

### Subtask:
Load the `train.csv` dataset into a pandas DataFrame.

**Reasoning**:
Import pandas and load the train.csv file into a DataFrame.
"""

import pandas as pd

df_train = pd.read_csv('C:/Users/HARSHA VARDHAN/OneDrive/Documents/kaggle/Mlalgorithms/Titanicdataset/test.csv')

"""## Explore the data (eda)

### Subtask:
Perform exploratory data analysis to understand the data, including displaying the first few rows, showing summary statistics, and checking for missing values.

**Reasoning**:
Perform exploratory data analysis by displaying the head, summary statistics, and checking for missing values.
"""

display(df_train.head())
display(df_train.describe())
display(df_train.isnull().sum())

"""## Data preprocessing

### Subtask:
Handle missing values, encode categorical variables, and drop unnecessary columns.

**Reasoning**:
Handle missing values, encode categorical variables, and drop unnecessary columns as per the instructions.
"""

df_train['Age'].fillna(df_train['Age'].median(), inplace=True)
df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)
df_train.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)
df_train = pd.get_dummies(df_train, columns=['Sex', 'Embarked'], drop_first=True)
display(df_train.head())
display(df_train.isnull().sum())

"""## Feature selection and splitting

### Subtask:
Select relevant features, define X (features) and y (target), and split the data into training and testing sets.

**Reasoning**:
Define the target variable and feature set, then split the data into training and testing sets.
"""

y = df_train['Survived']
X = df_train.drop('Survived', axis=1)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Model building

### Subtask:
Import and train a Decision Tree Classifier on the training data.

**Reasoning**:
Import the DecisionTreeClassifier and train the model.
"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

"""## Evaluation

### Subtask:
Predict on the test set and evaluate the model using accuracy, confusion matrix, and classification report.

**Reasoning**:
Make predictions on the test set and evaluate the model using accuracy, confusion matrix, and classification report.
"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

"""## Visualization

### Subtask:
Visualize the decision tree.

**Reasoning**:
Visualize the decision tree using plot_tree and display it.
"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(model, feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True, rounded=True, max_depth=3, fontsize=10)
plt.show()

"""## Print accuracy

### Subtask:
Print the final model accuracy and feature importances.

**Reasoning**:
Print the calculated accuracy and display the feature importances.
"""

print(f"Final Model Accuracy: {accuracy}")

feature_importances = pd.Series(model.feature_importances_, index=X.columns)
sorted_feature_importances = feature_importances.sort_values(ascending=False)

print("\nFeature Importances:")
display(sorted_feature_importances)

"""## Summary:

### Data Analysis Key Findings

*   The Decision Tree Classifier model achieved an accuracy of approximately 78.21% on the test set of the Titanic dataset.
*   The most important features for the model's prediction were 'Sex_male', 'Fare', and 'Age'.
*   The confusion matrix indicates that the model correctly predicted 84 non-survivors and 56 survivors in the test set.

### Insights or Next Steps

*   The model's performance is reasonable for this dataset, but there is room for improvement, particularly in reducing false positives and false negatives.
*   Further tuning of Decision Tree hyperparameters (e.g., `max_depth`, `min_samples_split`, `min_samples_leaf`) could potentially improve accuracy and generalization.

"""